{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing the pandas library for reading\nimport pandas as pd","metadata":{"id":"BxEEz3u8yisV","execution":{"iopub.status.busy":"2023-04-07T03:44:24.047110Z","iopub.execute_input":"2023-04-07T03:44:24.047437Z","iopub.status.idle":"2023-04-07T03:44:24.051303Z","shell.execute_reply.started":"2023-04-07T03:44:24.047411Z","shell.execute_reply":"2023-04-07T03:44:24.050318Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path='/kaggle/input/datacleaned/data (2).csv'","metadata":{"id":"_bSlBIyQ3Ozf","execution":{"iopub.status.busy":"2023-04-07T03:44:24.052924Z","iopub.execute_input":"2023-04-07T03:44:24.053209Z","iopub.status.idle":"2023-04-07T03:44:24.062578Z","shell.execute_reply.started":"2023-04-07T03:44:24.053186Z","shell.execute_reply":"2023-04-07T03:44:24.061159Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv(path)","metadata":{"id":"rgrdzkCr2bNZ","execution":{"iopub.status.busy":"2023-04-07T03:44:24.064263Z","iopub.execute_input":"2023-04-07T03:44:24.064820Z","iopub.status.idle":"2023-04-07T03:44:27.210606Z","shell.execute_reply.started":"2023-04-07T03:44:24.064791Z","shell.execute_reply":"2023-04-07T03:44:27.209951Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"ruVM2h73Moqq","outputId":"0cc1c53a-20a7-4112-f189-3c75a268be6f","execution":{"iopub.status.busy":"2023-04-07T03:44:27.211678Z","iopub.execute_input":"2023-04-07T03:44:27.212064Z","iopub.status.idle":"2023-04-07T03:44:27.214536Z","shell.execute_reply.started":"2023-04-07T03:44:27.212040Z","shell.execute_reply":"2023-04-07T03:44:27.214004Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Checking the info of the dataset\ndata1.info","metadata":{"id":"kLSwJ2rh3Vv-","outputId":"2da38cfd-1990-485d-a82e-12f2f4838b21","execution":{"iopub.status.busy":"2023-04-07T03:44:27.216327Z","iopub.execute_input":"2023-04-07T03:44:27.217153Z","iopub.status.idle":"2023-04-07T03:44:27.269151Z","shell.execute_reply.started":"2023-04-07T03:44:27.217131Z","shell.execute_reply":"2023-04-07T03:44:27.268076Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<bound method DataFrame.info of              id  target                                       comment_text  \\\n0        239579  0.4400  This is a great story. Man. I wonder if the pe...   \n1        239607  0.9125  Yet call out all Muslims for the acts of a few...   \n2        239644  0.0000  Because the people who drive cars more are the...   \n3        239653  0.3000  Mormons have had a complicated relationship wi...   \n4        239744  0.0000                       I'm doing the same thing! :)   \n...         ...     ...                                                ...   \n235082  6333915  0.3000  Xi and his comrades must be smirking over Trum...   \n235083  6333928  0.2000  My thought exactly.  The only people he hasn't...   \n235084  6333941  0.0000  I agree, Bill G\\nThe vote-buying has begun by ...   \n235085  6333947  0.0000  No, the probability of dying may be very, very...   \n235086  6333950  0.2000  Nah, I am too boring to parody.  This guy Camp...   \n\n        severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n0                     0        0                0       0       0      0   \n1                     0        0                1       1       0      0   \n2                     0        0                0       0       0      0   \n3                     0        0                0       0       0      0   \n4                     0        0                0       0       0      0   \n...                 ...      ...              ...     ...     ...    ...   \n235082                0        0                0       0       0      0   \n235083                0        0                0       0       0      0   \n235084                0        0                0       0       0      0   \n235085                0        0                0       0       0      0   \n235086                0        0                0       0       0      0   \n\n        atheist  ...  month  year  Season  Attack  Disability  Religion  Race  \\\n0             0  ...      1  2016  Summer       0           0         0     0   \n1             0  ...      1  2016  Summer       1           0         2     0   \n2             0  ...      1  2016  Summer       0           0         0     0   \n3             0  ...      1  2016  Summer       0           0         0     0   \n4             0  ...      1  2016  Summer       0           0         0     0   \n...         ...  ...    ...   ...     ...     ...         ...       ...   ...   \n235082        0  ...     11  2017  Spring       0           0         0     0   \n235083        0  ...     11  2017  Spring       0           0         0     0   \n235084        0  ...     11  2017  Spring       0           0         0     0   \n235085        0  ...     11  2017  Spring       0           0         0     0   \n235086        0  ...     11  2017  Spring       0           0         0     0   \n\n        Gender  sexual_orientation  Target  \n0            0                   0       0  \n1            0                   0       1  \n2            0                   0       0  \n3            0                   0       0  \n4            0                   0       0  \n...        ...                 ...     ...  \n235082       0                   0       0  \n235083       0                   0       0  \n235084       0                   0       0  \n235085       0                   0       0  \n235086       0                   0       0  \n\n[235087 rows x 60 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"#Step 1: NLP pre processing\n#Prepocessing the text (description column). i.e- removing the stop words and lemmatizing","metadata":{"id":"vcHy8Oz_3bbD","execution":{"iopub.status.busy":"2023-04-07T03:44:27.270649Z","iopub.execute_input":"2023-04-07T03:44:27.270945Z","iopub.status.idle":"2023-04-07T03:44:27.274837Z","shell.execute_reply.started":"2023-04-07T03:44:27.270918Z","shell.execute_reply":"2023-04-07T03:44:27.273681Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('omw-1.4')","metadata":{"id":"_jk2iiZs4Iqr","outputId":"b4f12f19-393c-47c1-ce0a-c1d7262ce07e","execution":{"iopub.status.busy":"2023-04-07T03:44:27.276020Z","iopub.execute_input":"2023-04-07T03:44:27.276263Z","iopub.status.idle":"2023-04-07T03:44:27.794814Z","shell.execute_reply.started":"2023-04-07T03:44:27.276242Z","shell.execute_reply":"2023-04-07T03:44:27.793933Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nimport string\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\n\ndef preprocess_text(text):\n      corpus=[]\n      #stem=PorterStemmer()\n      lem=WordNetLemmatizer()\n      for news in text:\n          words=[w for w in word_tokenize(news) if (w not in stop)]\n          \n          words = [lem.lemmatize(w) for w in words if len(w)>2]\n          words = [''.join(c for c in s if c not in string.punctuation) for s in words if s]\n          words = [word.lower() for word in words]\n          words = [word for word in words if word.isalpha()]\n          corpus.append(words) \n         \n      return corpus     \n      \ndata1['processed_comment_text']= preprocess_text(data1['comment_text'])","metadata":{"id":"QrYPa3cy3gWC","outputId":"5666b149-3f7e-40f2-d9d7-3324465370e0","execution":{"iopub.status.busy":"2023-04-07T03:44:27.795899Z","iopub.execute_input":"2023-04-07T03:44:27.796144Z","iopub.status.idle":"2023-04-07T03:47:14.829047Z","shell.execute_reply.started":"2023-04-07T03:44:27.796121Z","shell.execute_reply":"2023-04-07T03:47:14.828115Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"#Printing the first 5 elements of the dataset\ndata1.head()","metadata":{"id":"CLah3ndv5Fa1","outputId":"e496305a-59a1-4181-a3a6-ee9dc8adf31c","execution":{"iopub.status.busy":"2023-04-07T03:47:14.830293Z","iopub.execute_input":"2023-04-07T03:47:14.831062Z","iopub.status.idle":"2023-04-07T03:47:14.853265Z","shell.execute_reply.started":"2023-04-07T03:47:14.831032Z","shell.execute_reply":"2023-04-07T03:47:14.852386Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       id  target                                       comment_text  \\\n0  239579  0.4400  This is a great story. Man. I wonder if the pe...   \n1  239607  0.9125  Yet call out all Muslims for the acts of a few...   \n2  239644  0.0000  Because the people who drive cars more are the...   \n3  239653  0.3000  Mormons have had a complicated relationship wi...   \n4  239744  0.0000                       I'm doing the same thing! :)   \n\n   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n0                0        0                0       0       0      0        0   \n1                0        0                1       1       0      0        0   \n2                0        0                0       0       0      0        0   \n3                0        0                0       0       0      0        0   \n4                0        0                0       0       0      0        0   \n\n   ...  year  Season  Attack  Disability  Religion  Race  Gender  \\\n0  ...  2016  Summer       0           0         0     0       0   \n1  ...  2016  Summer       1           0         2     0       0   \n2  ...  2016  Summer       0           0         0     0       0   \n3  ...  2016  Summer       0           0         0     0       0   \n4  ...  2016  Summer       0           0         0     0       0   \n\n   sexual_orientation  Target  \\\n0                   0       0   \n1                   0       1   \n2                   0       0   \n3                   0       0   \n4                   0       0   \n\n                              processed_comment_text  \n0  [this, great, story, man, wonder, person, yell...  \n1  [yet, call, muslims, act, get, pilloried, okay...  \n2  [because, people, drive, car, one, cause, wear...  \n3  [mormons, complicated, relationship, federal, ...  \n4                                            [thing]  \n\n[5 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>...</th>\n      <th>year</th>\n      <th>Season</th>\n      <th>Attack</th>\n      <th>Disability</th>\n      <th>Religion</th>\n      <th>Race</th>\n      <th>Gender</th>\n      <th>sexual_orientation</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>239579</td>\n      <td>0.4400</td>\n      <td>This is a great story. Man. I wonder if the pe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2016</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[this, great, story, man, wonder, person, yell...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239607</td>\n      <td>0.9125</td>\n      <td>Yet call out all Muslims for the acts of a few...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2016</td>\n      <td>Summer</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[yet, call, muslims, act, get, pilloried, okay...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>239644</td>\n      <td>0.0000</td>\n      <td>Because the people who drive cars more are the...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2016</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[because, people, drive, car, one, cause, wear...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>239653</td>\n      <td>0.3000</td>\n      <td>Mormons have had a complicated relationship wi...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2016</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[mormons, complicated, relationship, federal, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>239744</td>\n      <td>0.0000</td>\n      <td>I'm doing the same thing! :)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2016</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[thing]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Viewing the processed text.\ndata1['processed_comment_text']","metadata":{"id":"oIMxzDn65MP9","outputId":"08874432-d30e-42af-a10f-6b911cfb254a","execution":{"iopub.status.busy":"2023-04-07T03:47:14.854767Z","iopub.execute_input":"2023-04-07T03:47:14.855319Z","iopub.status.idle":"2023-04-07T03:47:14.870260Z","shell.execute_reply.started":"2023-04-07T03:47:14.855288Z","shell.execute_reply":"2023-04-07T03:47:14.869429Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0         [this, great, story, man, wonder, person, yell...\n1         [yet, call, muslims, act, get, pilloried, okay...\n2         [because, people, drive, car, one, cause, wear...\n3         [mormons, complicated, relationship, federal, ...\n4                                                   [thing]\n                                ...                        \n235082    [comrade, must, smirking, trump, visit, after,...\n235083    [thought, exactly, the, people, nt, demonized,...\n235084    [agree, bill, the, votebuying, begun, ndp, nex...\n235085    [probability, dying, may, small, always, with,...\n235086    [nah, boring, parody, this, guy, campbe, magic...\nName: processed_comment_text, Length: 235087, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"#Finding and printing the maximum length of the sentence in the processed text\nlgt = []\nfor i in data1['processed_comment_text']:\n  lgt.append(len(i))\nprint('Maximum length of the sentence in processed text (in list) :',max(lgt))","metadata":{"id":"G8BwL01b5T2r","outputId":"6436878b-ecae-46b3-c1cc-ca2a963c2373","execution":{"iopub.status.busy":"2023-04-07T03:47:14.871489Z","iopub.execute_input":"2023-04-07T03:47:14.871814Z","iopub.status.idle":"2023-04-07T03:47:14.952258Z","shell.execute_reply.started":"2023-04-07T03:47:14.871784Z","shell.execute_reply":"2023-04-07T03:47:14.951159Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Maximum length of the sentence in processed text (in list) : 149\n","output_type":"stream"}]},{"cell_type":"code","source":"comment_processed = []\nfor i in range(len(data1['processed_comment_text'])):\n   comment_processed.append(' '.join(wrd for wrd in data1.iloc[:,60][i]))\ndata1['comment_text_processed'] = comment_processed\ndata1.head()","metadata":{"id":"74l_saXD5bac","outputId":"298a9ca2-cbbb-45e3-9230-093324a2578e","execution":{"iopub.status.busy":"2023-04-07T03:47:14.953751Z","iopub.execute_input":"2023-04-07T03:47:14.954083Z","iopub.status.idle":"2023-04-07T03:47:27.679821Z","shell.execute_reply.started":"2023-04-07T03:47:14.954052Z","shell.execute_reply":"2023-04-07T03:47:27.678891Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       id  target                                       comment_text  \\\n0  239579  0.4400  This is a great story. Man. I wonder if the pe...   \n1  239607  0.9125  Yet call out all Muslims for the acts of a few...   \n2  239644  0.0000  Because the people who drive cars more are the...   \n3  239653  0.3000  Mormons have had a complicated relationship wi...   \n4  239744  0.0000                       I'm doing the same thing! :)   \n\n   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n0                0        0                0       0       0      0        0   \n1                0        0                1       1       0      0        0   \n2                0        0                0       0       0      0        0   \n3                0        0                0       0       0      0        0   \n4                0        0                0       0       0      0        0   \n\n   ...  Season  Attack  Disability  Religion  Race  Gender  \\\n0  ...  Summer       0           0         0     0       0   \n1  ...  Summer       1           0         2     0       0   \n2  ...  Summer       0           0         0     0       0   \n3  ...  Summer       0           0         0     0       0   \n4  ...  Summer       0           0         0     0       0   \n\n   sexual_orientation  Target  \\\n0                   0       0   \n1                   0       1   \n2                   0       0   \n3                   0       0   \n4                   0       0   \n\n                              processed_comment_text  \\\n0  [this, great, story, man, wonder, person, yell...   \n1  [yet, call, muslims, act, get, pilloried, okay...   \n2  [because, people, drive, car, one, cause, wear...   \n3  [mormons, complicated, relationship, federal, ...   \n4                                            [thing]   \n\n                              comment_text_processed  \n0  this great story man wonder person yelled shut...  \n1  yet call muslims act get pilloried okay smear ...  \n2  because people drive car one cause wear tear r...  \n3       mormons complicated relationship federal law  \n4                                              thing  \n\n[5 rows x 62 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>...</th>\n      <th>Season</th>\n      <th>Attack</th>\n      <th>Disability</th>\n      <th>Religion</th>\n      <th>Race</th>\n      <th>Gender</th>\n      <th>sexual_orientation</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n      <th>comment_text_processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>239579</td>\n      <td>0.4400</td>\n      <td>This is a great story. Man. I wonder if the pe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[this, great, story, man, wonder, person, yell...</td>\n      <td>this great story man wonder person yelled shut...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239607</td>\n      <td>0.9125</td>\n      <td>Yet call out all Muslims for the acts of a few...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Summer</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[yet, call, muslims, act, get, pilloried, okay...</td>\n      <td>yet call muslims act get pilloried okay smear ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>239644</td>\n      <td>0.0000</td>\n      <td>Because the people who drive cars more are the...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[because, people, drive, car, one, cause, wear...</td>\n      <td>because people drive car one cause wear tear r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>239653</td>\n      <td>0.3000</td>\n      <td>Mormons have had a complicated relationship wi...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[mormons, complicated, relationship, federal, ...</td>\n      <td>mormons complicated relationship federal law</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>239744</td>\n      <td>0.0000</td>\n      <td>I'm doing the same thing! :)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Summer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[thing]</td>\n      <td>thing</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 62 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Number of characters present in each sentence\n#data1['comment_text'].str.len().hist();","metadata":{"id":"cpjwOwXT6izp","execution":{"iopub.status.busy":"2023-03-17T00:00:30.883150Z","iopub.execute_input":"2023-03-17T00:00:30.884534Z","iopub.status.idle":"2023-03-17T00:00:30.889741Z","shell.execute_reply.started":"2023-03-17T00:00:30.884487Z","shell.execute_reply":"2023-03-17T00:00:30.888256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#data1['comment_text_processed'].str.len().hist();","metadata":{"id":"d5m6QxV86wRI","execution":{"iopub.status.busy":"2023-03-17T00:00:30.892044Z","iopub.execute_input":"2023-03-17T00:00:30.892947Z","iopub.status.idle":"2023-03-17T00:00:30.905638Z","shell.execute_reply.started":"2023-03-17T00:00:30.892903Z","shell.execute_reply":"2023-03-17T00:00:30.904137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of words appearing in each description\n#data1['comment_text'].str.split().map(lambda x: len(x)).hist();","metadata":{"id":"tRujOZ9r68oy","execution":{"iopub.status.busy":"2023-03-17T00:00:30.907839Z","iopub.execute_input":"2023-03-17T00:00:30.908294Z","iopub.status.idle":"2023-03-17T00:00:30.919590Z","shell.execute_reply.started":"2023-03-17T00:00:30.908249Z","shell.execute_reply":"2023-03-17T00:00:30.917936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of words appearing in each description\n#data1['comment_text_processed'].str.split().map(lambda x: len(x)).hist();","metadata":{"id":"6xt85sNq7JMN","execution":{"iopub.status.busy":"2023-03-17T00:00:30.921223Z","iopub.execute_input":"2023-03-17T00:00:30.922573Z","iopub.status.idle":"2023-03-17T00:00:30.936691Z","shell.execute_reply.started":"2023-03-17T00:00:30.922527Z","shell.execute_reply":"2023-03-17T00:00:30.935527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#Average word length\nimport numpy as np\ndata1['comment_text'].str.split().apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist();'''","metadata":{"id":"twtqQhGW7Q8t","outputId":"19ebca6f-a0bc-470c-8cdb-434662973dd3","execution":{"iopub.status.busy":"2023-03-17T00:00:30.938521Z","iopub.execute_input":"2023-03-17T00:00:30.939147Z","iopub.status.idle":"2023-03-17T00:00:30.952430Z","shell.execute_reply.started":"2023-03-17T00:00:30.939109Z","shell.execute_reply":"2023-03-17T00:00:30.951379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''data1['comment_text_processed'].str.split().apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist();'''","metadata":{"id":"V0XL6N597d3A","outputId":"ab97bca0-2537-4ac4-c617-abe7494d7ff9","execution":{"iopub.status.busy":"2023-03-17T00:00:30.953891Z","iopub.execute_input":"2023-03-17T00:00:30.954519Z","iopub.status.idle":"2023-03-17T00:00:30.968353Z","shell.execute_reply.started":"2023-03-17T00:00:30.954481Z","shell.execute_reply":"2023-03-17T00:00:30.967435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#N-gram analysis\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport seaborn as sns\n\ndef plot_top_ngrams_barchart(text, n=2):\n    stop=set(stopwords.words('english'))\n\n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n\n    def get_top_ngram(corpus, n=None):\n        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        words_freq = [(word, sum_words[0, idx]) \n                      for word, idx in vec.vocabulary_.items()]\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n        return words_freq[:10]\n\n    top_n_bigrams= get_top_ngram(text,n)[:10]\n    x,y=map(list,zip(*top_n_bigrams))\n    sns.barplot(x=y,y=x)'''","metadata":{"id":"26Kuz42B7iNm","outputId":"84084a0f-66d8-4210-8bfe-621419e38cb1","execution":{"iopub.status.busy":"2023-03-17T00:00:30.969867Z","iopub.execute_input":"2023-03-17T00:00:30.970508Z","iopub.status.idle":"2023-03-17T00:00:30.983444Z","shell.execute_reply.started":"2023-03-17T00:00:30.970470Z","shell.execute_reply":"2023-03-17T00:00:30.981953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bigram analysis\n#plot_top_ngrams_barchart(data1['comment_text_processed'],2)","metadata":{"id":"-C2OEQVV7zBj","execution":{"iopub.status.busy":"2023-03-17T00:00:30.985474Z","iopub.execute_input":"2023-03-17T00:00:30.985943Z","iopub.status.idle":"2023-03-17T00:00:30.996339Z","shell.execute_reply.started":"2023-03-17T00:00:30.985904Z","shell.execute_reply":"2023-03-17T00:00:30.994196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trigram analysis\n#plot_top_ngrams_barchart(data1['comment_text_processed'],3)","metadata":{"id":"rSw6kveH73H0","execution":{"iopub.status.busy":"2023-03-17T00:00:30.998476Z","iopub.execute_input":"2023-03-17T00:00:30.998961Z","iopub.status.idle":"2023-03-17T00:00:31.008423Z","shell.execute_reply.started":"2023-03-17T00:00:30.998924Z","shell.execute_reply":"2023-03-17T00:00:31.007421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For n=4\n#plot_top_ngrams_barchart(data1['comment_text_processed'],4)","metadata":{"id":"LYJLxZOx8BC8","execution":{"iopub.status.busy":"2023-03-17T00:00:31.010201Z","iopub.execute_input":"2023-03-17T00:00:31.011362Z","iopub.status.idle":"2023-03-17T00:00:31.022301Z","shell.execute_reply.started":"2023-03-17T00:00:31.011319Z","shell.execute_reply":"2023-03-17T00:00:31.020960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#Wordcloud\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=100,\n        max_font_size=30, \n        scale=3,\n        random_state=1)\ndesc = data1['comment_text_processed']\nwordcloud=wordcloud.generate(str(desc))\n\nfig = plt.figure(1, figsize=(12, 12)) \nplt.axis('off')\n \nplt.imshow(wordcloud)\nplt.show()'''","metadata":{"id":"CRPJ6lVe8Ka8","outputId":"9dfb6e17-9ef2-4585-f8cc-d5fc40f68f8e","execution":{"iopub.status.busy":"2023-03-17T00:00:31.024449Z","iopub.execute_input":"2023-03-17T00:00:31.024973Z","iopub.status.idle":"2023-03-17T00:00:31.038007Z","shell.execute_reply.started":"2023-03-17T00:00:31.024935Z","shell.execute_reply":"2023-03-17T00:00:31.036511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test= '/kaggle/input/testda/test.csv'","metadata":{"id":"-jB3_QyL-ak-","execution":{"iopub.status.busy":"2023-03-17T00:00:31.047423Z","iopub.execute_input":"2023-03-17T00:00:31.048500Z","iopub.status.idle":"2023-03-17T00:00:31.055484Z","shell.execute_reply.started":"2023-03-17T00:00:31.048430Z","shell.execute_reply":"2023-03-17T00:00:31.054018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"p_KMjfxHe5vm","execution":{"iopub.status.busy":"2023-03-17T00:00:31.057540Z","iopub.execute_input":"2023-03-17T00:00:31.058444Z","iopub.status.idle":"2023-03-17T00:00:31.073529Z","shell.execute_reply.started":"2023-03-17T00:00:31.058360Z","shell.execute_reply":"2023-03-17T00:00:31.072245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(test)","metadata":{"id":"P9HmHdK8-hqU","execution":{"iopub.status.busy":"2023-03-17T00:00:31.075076Z","iopub.execute_input":"2023-03-17T00:00:31.076474Z","iopub.status.idle":"2023-03-17T00:00:32.014571Z","shell.execute_reply.started":"2023-03-17T00:00:31.076398Z","shell.execute_reply":"2023-03-17T00:00:32.012838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"id":"sJ72wvRn-rzs","outputId":"f4bf769b-ae73-4fc1-a4a5-242abfade67a","execution":{"iopub.status.busy":"2023-03-17T00:00:32.016261Z","iopub.execute_input":"2023-03-17T00:00:32.016746Z","iopub.status.idle":"2023-03-17T00:00:32.029153Z","shell.execute_reply.started":"2023-03-17T00:00:32.016698Z","shell.execute_reply":"2023-03-17T00:00:32.028248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_column = data1['comment_text_processed']\n\n\n# Divide the text column into 5 parts\ntext_column_parts = np.array_split(text_column, 1)\n\n# Initialize an empty list to store the result\nresult = []\n\n# Perform Tf-Idf on each part\nfor part in text_column_parts:\n    vectorizer = TfidfVectorizer()\n    sparse_matrix = vectorizer.fit_transform(part).toarray()\n    result.append(sparse_matrix)\n    \n# Combine the result into a single sparse matrix\nfinal_result = result[0]\nfor i in range(1, len(result)):\n    final_result = final_result + result[i]'''","metadata":{"id":"xXqMgvo3c8tu","outputId":"5989669c-df4b-4cb8-a13d-526af1d425d1","execution":{"iopub.status.busy":"2023-03-17T00:00:32.030680Z","iopub.execute_input":"2023-03-17T00:00:32.031284Z","iopub.status.idle":"2023-03-17T00:00:32.043665Z","shell.execute_reply.started":"2023-03-17T00:00:32.031245Z","shell.execute_reply":"2023-03-17T00:00:32.042217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''df = pd.DataFrame(final_result)\ndf.head()'''","metadata":{"id":"0viHIXMkc8tu","outputId":"246e0546-f92e-43e6-e0e4-7e61d4e855c1","execution":{"iopub.status.busy":"2023-03-17T00:00:32.045342Z","iopub.execute_input":"2023-03-17T00:00:32.045747Z","iopub.status.idle":"2023-03-17T00:00:32.057584Z","shell.execute_reply.started":"2023-03-17T00:00:32.045715Z","shell.execute_reply":"2023-03-17T00:00:32.056064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''svc = LinearSVC( max_iter=2500)\nsvc.fit(df, y)\n\nacc_svc_t = model_c.score(df, y)\nprint(\"Train accuracy of the SVC model : {:.2f}\".format(acc_svc_t*100))'''","metadata":{"id":"sXEOHp92c8tv","outputId":"aebaf759-d7cb-48c3-e6e1-5a6b7c3eaeab","execution":{"iopub.status.busy":"2023-03-17T00:00:32.059444Z","iopub.execute_input":"2023-03-17T00:00:32.059974Z","iopub.status.idle":"2023-03-17T00:00:32.071701Z","shell.execute_reply.started":"2023-03-17T00:00:32.059923Z","shell.execute_reply":"2023-03-17T00:00:32.070298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['processed_comment_text']= preprocess_text(test['comment_text'])\ntest.head()","metadata":{"id":"HdPNgcubc8tv","outputId":"c383d6f8-e9a4-45b9-8be9-0071690260df","execution":{"iopub.status.busy":"2023-03-17T00:00:32.074353Z","iopub.execute_input":"2023-03-17T00:00:32.074954Z","iopub.status.idle":"2023-03-17T00:01:48.739508Z","shell.execute_reply.started":"2023-03-17T00:00:32.074900Z","shell.execute_reply":"2023-03-17T00:01:48.738492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_processed = []\nfor i in range(len(test['processed_comment_text'])):\n   comment_processed.append(' '.join(wrd for wrd in test.iloc[:,2][i]))\ntest['comment_text_processed'] = comment_processed\ntest.head()","metadata":{"id":"vljeQ663c8tv","outputId":"6bc24ca5-4988-4608-ee3a-1808d9a4df79","execution":{"iopub.status.busy":"2023-03-17T00:01:48.741310Z","iopub.execute_input":"2023-03-17T00:01:48.741747Z","iopub.status.idle":"2023-03-17T00:01:55.039564Z","shell.execute_reply.started":"2023-03-17T00:01:48.741707Z","shell.execute_reply":"2023-03-17T00:01:55.037618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeature = data1[['comment_text_processed']]\noutput = data1[['Target']]\nX_train, X_cv, y_train, y_cv = train_test_split(feature, output)\n\nprint(X_train.shape)\nprint(X_cv.shape)\nprint(y_train.shape)\nprint(y_cv.shape)","metadata":{"id":"IbnecF9gc8tv","outputId":"afcbfc09-3b72-464c-f077-a6e7b53d01d8","execution":{"iopub.status.busy":"2023-03-17T00:01:55.041462Z","iopub.execute_input":"2023-03-17T00:01:55.042649Z","iopub.status.idle":"2023-03-17T00:01:55.182922Z","shell.execute_reply.started":"2023-03-17T00:01:55.042588Z","shell.execute_reply":"2023-03-17T00:01:55.181619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test[['comment_text_processed']]\nX_test.head()","metadata":{"id":"Hutvsqlyc8tv","outputId":"bc68c792-5bf5-47e9-e942-574803dd6e02","execution":{"iopub.status.busy":"2023-03-17T00:01:55.185029Z","iopub.execute_input":"2023-03-17T00:01:55.185400Z","iopub.status.idle":"2023-03-17T00:01:55.215906Z","shell.execute_reply.started":"2023-03-17T00:01:55.185368Z","shell.execute_reply":"2023-03-17T00:01:55.214266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncnt_vec = CountVectorizer(ngram_range=(1,2), max_features=30000)\nbow_train = cnt_vec.fit_transform(X_train['comment_text_processed'])\nbow_cv = cnt_vec.transform(X_cv['comment_text_processed'])\nbow_test = cnt_vec.transform(X_test['comment_text_processed'])\n\nprint(bow_train.shape)\nprint(bow_cv.shape)\nprint(bow_test.shape)","metadata":{"id":"eDKNlxrgc8tv","outputId":"b8d70b9b-0d80-490b-c3b6-bbc65828bdac","execution":{"iopub.status.busy":"2023-03-17T00:01:55.217791Z","iopub.execute_input":"2023-03-17T00:01:55.218367Z","iopub.status.idle":"2023-03-17T00:02:37.664119Z","shell.execute_reply.started":"2023-03-17T00:01:55.218307Z","shell.execute_reply":"2023-03-17T00:02:37.662498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.svm import LinearSVC\n\nsvc = LinearSVC( max_iter=100)\nsvc.fit(bow_train, y_train)\nacc_svc_tr = svc.score(bow_train, y_train)\nprint(\"Train accuracy of the SVC model : {:.2f}\".format(acc_svc_tr*100))","metadata":{"id":"72-I7s57c8tw","outputId":"35d6eb88-1be0-49b3-8abf-5ce785a139d7","execution":{"iopub.status.busy":"2023-03-17T00:02:37.665883Z","iopub.execute_input":"2023-03-17T00:02:37.666936Z","iopub.status.idle":"2023-03-17T00:02:45.440867Z","shell.execute_reply.started":"2023-03-17T00:02:37.666895Z","shell.execute_reply":"2023-03-17T00:02:45.439450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"geBlClmkc8tw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svc.predict(bow_cv)\nprint(classification_report(y_cv, predictions))","metadata":{"id":"MNe0K6P1c8tw","outputId":"ecc194bc-d059-4808-9736-ccbc29ba74c6","execution":{"iopub.status.busy":"2023-03-17T00:02:45.443095Z","iopub.execute_input":"2023-03-17T00:02:45.444039Z","iopub.status.idle":"2023-03-17T00:02:45.539383Z","shell.execute_reply.started":"2023-03-17T00:02:45.443985Z","shell.execute_reply":"2023-03-17T00:02:45.537835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\n\n\n# balance the dataset using SMOTE\nsm = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = sm.fit_resample(bow_train, y_train)\n\n# train a linear SVM on the balanced dataset\nclf = LinearSVC()\nclf.fit(X_train_resampled, y_train_resampled)\n\n# evaluate the model on the test set\ny_pred = clf.predict(bow_test)\n\n","metadata":{"id":"eiROLceQc8tw","outputId":"98a66bb8-41cd-46c0-ab94-7b30ae1d1b3c","execution":{"iopub.status.busy":"2023-03-17T00:02:45.541656Z","iopub.execute_input":"2023-03-17T00:02:45.542443Z","iopub.status.idle":"2023-03-17T00:04:59.004387Z","shell.execute_reply.started":"2023-03-17T00:02:45.542373Z","shell.execute_reply":"2023-03-17T00:04:59.003060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"id":"T_iSrBiLc8tw","outputId":"d7385ac1-5935-4894-8f88-d80ce799c98f","execution":{"iopub.status.busy":"2023-03-17T00:04:59.006140Z","iopub.execute_input":"2023-03-17T00:04:59.006579Z","iopub.status.idle":"2023-03-17T00:04:59.014829Z","shell.execute_reply.started":"2023-03-17T00:04:59.006542Z","shell.execute_reply":"2023-03-17T00:04:59.013504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_svc_tr1 = clf.score(X_train_resampled, y_train_resampled)\nprint(\"Train accuracy of the SVC model : {:.2f}\".format(acc_svc_tr1*100))","metadata":{"id":"16_rs2LIc8tx","outputId":"88364995-dbe4-4d0c-cd0c-2ec6a982e9f5","execution":{"iopub.status.busy":"2023-03-17T00:04:59.016742Z","iopub.execute_input":"2023-03-17T00:04:59.017242Z","iopub.status.idle":"2023-03-17T00:04:59.099620Z","shell.execute_reply.started":"2023-03-17T00:04:59.017196Z","shell.execute_reply":"2023-03-17T00:04:59.098365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1= pd.DataFrame(y_pred, columns=['Target'])","metadata":{"id":"q7vLubfqc8tx","execution":{"iopub.status.busy":"2023-03-17T00:04:59.101379Z","iopub.execute_input":"2023-03-17T00:04:59.101806Z","iopub.status.idle":"2023-03-17T00:04:59.107975Z","shell.execute_reply.started":"2023-03-17T00:04:59.101770Z","shell.execute_reply":"2023-03-17T00:04:59.106931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1.head()","metadata":{"id":"FnK4ubtZc8tx","outputId":"42e8760f-3d25-4934-ee60-3d2da4dea93f","execution":{"iopub.status.busy":"2023-03-17T00:04:59.110044Z","iopub.execute_input":"2023-03-17T00:04:59.110444Z","iopub.status.idle":"2023-03-17T00:04:59.128595Z","shell.execute_reply.started":"2023-03-17T00:04:59.110382Z","shell.execute_reply":"2023-03-17T00:04:59.127246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = pd.DataFrame(y_pred1, columns=['Target'])","metadata":{"id":"p1UYAnIIc8tx","outputId":"4cb87bbc-f12f-4ad3-bf17-949917a85ee4","execution":{"iopub.status.busy":"2023-03-17T00:04:59.130297Z","iopub.execute_input":"2023-03-17T00:04:59.130793Z","iopub.status.idle":"2023-03-17T00:04:59.140033Z","shell.execute_reply.started":"2023-03-17T00:04:59.130738Z","shell.execute_reply":"2023-03-17T00:04:59.138635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"id":"nF6PGckhc8tx","outputId":"70ad10a2-7280-4a17-9868-cb5370bcdce6","execution":{"iopub.status.busy":"2023-03-17T00:04:59.141560Z","iopub.execute_input":"2023-03-17T00:04:59.141931Z","iopub.status.idle":"2023-03-17T00:04:59.156091Z","shell.execute_reply.started":"2023-03-17T00:04:59.141900Z","shell.execute_reply":"2023-03-17T00:04:59.154845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions1 = svc.predict(bow_test)\nprint(classification_report(y_test, predictions1))","metadata":{"id":"5ITg9ArOc8ty","outputId":"e6e371c5-317d-437f-9d4f-caa96fd583b2","execution":{"iopub.status.busy":"2023-03-17T00:04:59.157322Z","iopub.execute_input":"2023-03-17T00:04:59.157788Z","iopub.status.idle":"2023-03-17T00:04:59.327535Z","shell.execute_reply.started":"2023-03-17T00:04:59.157752Z","shell.execute_reply":"2023-03-17T00:04:59.326159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nunique_counts = y_train_resampled.nunique()\n\nprint(unique_counts)","metadata":{"id":"0uDi-PLVc8ty","outputId":"6ca8e4e6-d85c-4147-cccc-0d468e082faf","execution":{"iopub.status.busy":"2023-03-17T00:04:59.329618Z","iopub.execute_input":"2023-03-17T00:04:59.330139Z","iopub.status.idle":"2023-03-17T00:04:59.343035Z","shell.execute_reply.started":"2023-03-17T00:04:59.330093Z","shell.execute_reply":"2023-03-17T00:04:59.341370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n# Load your CSV file into a pandas DataFrame\n\n# Split the DataFrame into a text array and a label array\ntext_data = X_train['comment_text_processed'].to_numpy()\nlabel_data = y_train['Target'].to_numpy()\n\n# Define the text vectorizer layer\nvectorizer = TextVectorization(\n    max_tokens=10000, # maximum number of unique words to keep in the vocabulary\n    output_mode='int', # output integer indices for each word in the vocabulary\n    output_sequence_length=500 # maximum sequence length to pad/truncate all input sequences to\n)\n\n# Fit the text vectorizer layer to your text data\nvectorizer.adapt(text_data)\n\n# Convert the text data to preprocessed integer sequences\ntext_data = vectorizer(text_data)\n\n# Define the LSTM model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=len(vectorizer.get_vocabulary()), # size of the vocabulary\n        output_dim=128, # size of the embedding space\n        mask_zero=True # use masking to handle variable-length input sequences\n    ),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dropout(0.5), # add a dropout layer to reduce overfitting\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid') # binary classification output layer\n])\n\n# Compile the model\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n# Train the model\nmodel.fit(\n    text_data, label_data,\n    batch_size=32,\n    epochs=10,\n    validation_split=0.2\n)\n","metadata":{"id":"LxeoOadYc8ty","outputId":"2e6cbab7-8fa8-40d8-8c78-731ea0d3e8a8","execution":{"iopub.status.busy":"2023-03-17T18:39:21.108143Z","iopub.execute_input":"2023-03-17T18:39:21.109113Z","iopub.status.idle":"2023-03-17T18:39:29.511109Z","shell.execute_reply.started":"2023-03-17T18:39:21.108980Z","shell.execute_reply":"2023-03-17T18:39:29.509689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout","metadata":{"id":"L0ikUyMAzEvz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n","metadata":{"id":"hKciLprlzWJW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"id":"dOqGKGASzl-O","outputId":"24f69b64-2a05-4fa5-a199-db5882f6b137","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"id":"d3rBqUkO0zk3","outputId":"4d551569-45c4-437e-90fc-6048d4a9135f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(X_test['comment_text_processed'])\n","metadata":{"id":"cztiQ3yjyXrR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data1 = X_test['comment_text_processed']\nlabel_data1 = y_test['Target']\n\n# Define the text vectorizer layer\nvectorizer1 = TextVectorization(\n    max_tokens=10000, # maximum number of unique words to keep in the vocabulary\n    output_mode='int', # output integer indices for each word in the vocabulary\n    output_sequence_length=500 # maximum sequence length to pad/truncate all input sequences to\n)\n\n# Fit the text vectorizer layer to your text data\nvectorizer1.adapt(text_data1)\n\n# Convert the text data to preprocessed integer sequences\ntext_data1 = vectorizer(text_data1)","metadata":{"id":"MUesxjer1Klx","outputId":"b4b58e92-028f-4d1c-c054-7e8cfcce07c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences","metadata":{"id":"2ME8Z6Wd0IHv","outputId":"807b7fe5-4662-4bb0-de10-6d95323d1f9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_data1","metadata":{"id":"ycorwWf925aD","outputId":"85f7c4f0-0d8a-4169-dd69-d389be3dfa14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_loss, test_acc = model.evaluate(text_data1, label_data1)\nprint('Test accuracy:', test_acc)","metadata":{"id":"wBdVbE7w0q9T","outputId":"641f2318-a127-4f7a-8841-82840f0bb40d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Load your original data into a pandas data frame\n\n\n# Separate the text and label data\n\ntext_data3 = data1['comment_text_processed']\nlabel_data3 = data1['Target']\n\n# Encode the labels\n\n\n\n# Vectorize the text data\n\nvectorizer = TfidfVectorizer()\ntext_data_vec = vectorizer.fit_transform(text_data3)\n\n# Convert the text data to a dense numpy array\n\ntext_data_arr = text_data_vec.toarray()\n\n# Apply SMOTE to the text and label data\n\nsmote = SMOTE(random_state=42)\ntext_data_smote, label_data_smote = smote.fit_resample(text_data_arr, label_data3)\n\n# Pad the text data so that all sequences are of the same length\n\ntext_data_padded = pad_sequences(text_data_smote, padding='post')\n\n# Scale the data\n\nscaler = StandardScaler()\ntext_data_scaled = scaler.fit_transform(text_data_padded)\n\n# Split the data into training and testing sets\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(text_data_scaled, label_data_smote, test_size=0.2, random_state=42)\n\n# Define and train the LSTM model\n\n...\n\n# Evaluate the model on the test set\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)\n","metadata":{"id":"8exaqjQZ56es","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\n\n\n\n# Split the DataFrame into a text array and a label array\ntext_data1 = data1['comment_text_processed'].to_numpy()\nlabel_data1 = data1['Target'].to_numpy()\n\n# Define the text vectorizer layer\nvectorizer = TextVectorization(\n    max_tokens=10000, # maximum number of unique words to keep in the vocabulary\n    output_mode='int', # output integer indices for each word in the vocabulary\n    output_sequence_length=500 # maximum sequence length to pad/truncate all input sequences to\n)\n\n# Fit the text vectorizer layer to your text data\nvectorizer.adapt(text_data1)\n\n# Convert the text data to preprocessed integer sequences\ntext_data1 = vectorizer(text_data1)\n\n# Apply SMOTE to the text and label data\n\nsmote = SMOTE(random_state=42)\ntext_data_smote, label_data_smote = smote.fit_resample(text_data1.reshape(-1, 1), label_data1)\n\n\n\n# Encode the labels\n\n\n\n# Split the data into training and testing sets\n\nX_train4, X_test4, y_train4, y_test4 = train_test_split(text_data_smote, label_data_smote, test_size=0.2, random_state=42)\n\n# Define and train the LSTM model\n\n# Define the LSTM model\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=len(vectorizer.get_vocabulary()), # size of the vocabulary\n        output_dim=128, # size of the embedding space\n        mask_zero=True # use masking to handle variable-length input sequences\n    ),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dropout(0.5), # add a dropout layer to reduce overfitting\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid') # binary classification output layer\n])\n\n# Compile the model\nmodel1.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n# Train the model\nmodel1.fit(\n    X_train4,y_train4,\n    batch_size=32,\n    epochs=10,\n    validation_split=0.2\n)\n\n# Evaluate the model on the test set\n\ntest_loss1, test_acc1 = model1.evaluate(text_data1, label_data1)\nprint('Test accuracy:', test_acc1)\n","metadata":{"id":"yhGevb-U7Q7k","outputId":"8f0520fe-f6e5-40aa-809b-47dd68d67c05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nimport tensorflow as tf\n\n# Split the DataFrame into a text array and a label array\ntext_data1 = data1['comment_text_processed'].to_numpy()\nlabel_data1 = data1['Target'].to_numpy()\n\n# Define the text vectorizer layer\nvectorizer = TextVectorization(\n    max_tokens=10000, # maximum number of unique words to keep in the vocabulary\n    output_mode='int', # output integer indices for each word in the vocabulary\n    output_sequence_length=500 # maximum sequence length to pad/truncate all input sequences to\n)\n\n# Fit the text vectorizer layer to your text data\nvectorizer.adapt(text_data1)\n\n# Convert the text data to preprocessed integer sequences\ntext_data1 = vectorizer(text_data1)\n\n# Apply SMOTE to the text and label data\nsmote = SMOTE(random_state=42)\ntext_data_smote, label_data_smote = smote.fit_resample(text_data1, label_data1)\n\n# Split the data into training and testing sets\nX_train4, X_test4, y_train4, y_test4 = train_test_split(text_data_smote, label_data_smote, test_size=0.2, random_state=42)\n\n# Define and train the LSTM model\n\n# Define the LSTM model\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=len(vectorizer.get_vocabulary()), # size of the vocabulary\n        output_dim=128, # size of the embedding space\n        mask_zero=True # use masking to handle variable-length input sequences\n    ),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dropout(0.5), # add a dropout layer to reduce overfitting\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid') # binary classification output layer\n])\n\n# Compile the model\nmodel1.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n# Train the model\nmodel1.fit(\n    X_train4,y_train4,\n    batch_size=32,\n    epochs=3,\n    validation_split=0.2\n)\n\n# Evaluate the model on the test set\ntest_loss1, test_acc1 = model1.evaluate(X_train4, y_train4)\nprint('Test accuracy:', test_acc1)\n","metadata":{"id":"O2FUjt1vC4bc","outputId":"6f125c55-5ce0-4dba-e6ae-c14b80a4f2e0","execution":{"iopub.status.busy":"2023-04-07T06:09:27.581887Z","iopub.execute_input":"2023-04-07T06:09:27.582219Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/3\n8398/8398 [==============================] - 7966s 948ms/step - loss: 0.4183 - accuracy: 0.8059 - val_loss: 0.2956 - val_accuracy: 0.8735\nEpoch 2/3\n8398/8398 [==============================] - 8427s 1s/step - loss: 0.2542 - accuracy: 0.8931 - val_loss: 0.2528 - val_accuracy: 0.8923\nEpoch 3/3\n8398/8398 [==============================] - 8289s 987ms/step - loss: 0.1974 - accuracy: 0.9193 - val_loss: 0.2656 - val_accuracy: 0.8889\n  907/10498 [=>............................] - ETA: 46:08 - loss: 0.1477 - accuracy: 0.9420","output_type":"stream"}]},{"cell_type":"code","source":"#Bilstm model\nimport numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n\n\n\n# Separate the text and label data\n\ntext_data = data1['comment_text_processed']\nlabel_data = data1['Target']\n\n\n\n\n# Define the text tokenizer\n\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(text_data)\n\n# Convert the text data to integer sequences and pad them to the same length\n\ntext_data_seq = tokenizer.texts_to_sequences(text_data)\ntext_data_padded = pad_sequences(text_data_seq, padding='post', maxlen=500)\n\n# Apply SMOTE to the text and label data\n\nsmote = SMOTE(random_state=42)\ntext_data_smote, label_data_smote = smote.fit_resample(text_data_padded, label_data)\n\n# Split the data into training and testing sets\n\nX_train, X_cv8, y_train, y_cv8 = train_test_split(text_data_smote, label_data_smote, test_size=0.2, random_state=42)\n\n# Define and train the BiLSTM model\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=10000, output_dim=128, input_length=500))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, validation_data=(X_cv8, y_cv8), epochs=1, batch_size=32)\n\n# Evaluate the model on the test set\n\ntrain_loss, test_acc = model.evaluate(X_cv8, y_cv8)\nprint('Test accuracy:', test_acc)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T15:56:08.549301Z","iopub.execute_input":"2023-04-01T15:56:08.549793Z","iopub.status.idle":"2023-04-01T17:17:44.586828Z","shell.execute_reply.started":"2023-04-01T15:56:08.549758Z","shell.execute_reply":"2023-04-01T17:17:44.584870Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"10498/10498 [==============================] - 4584s 436ms/step - loss: 0.3848 - accuracy: 0.8220 - val_loss: 0.2989 - val_accuracy: 0.8696\n2625/2625 [==============================] - 255s 97ms/step - loss: 0.2989 - accuracy: 0.8696\nTest accuracy: 0.8695610761642456\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\n\n\n# Define the BiLSTM model as a function\ndef create_bilstm_model(input_dim, output_dim, mask_zero, lstm_units, dropout_rate, dense_units, optimizer):\n    model = Sequential([\n        Embedding(\n            input_dim=input_dim,\n            output_dim=output_dim,\n            mask_zero=mask_zero\n        ),\n        Bidirectional(LSTM(lstm_units)),\n        tf.keras.layers.Dropout(dropout_rate),\n        Dense(dense_units, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    return model\n\n\n# Split the DataFrame into a text array and a label array\ntext_data = data1['comment_text_processed'].to_numpy()\nlabel_data = data1['Target'].to_numpy()\n\n# Define the text vectorizer layer\nvectorizer = TextVectorization(\n    max_tokens=10000,\n    output_mode='int',\n    output_sequence_length=500\n)\n\n# Fit the text vectorizer layer to your text data\nvectorizer.adapt(text_data)\n\n# Convert the text data to preprocessed integer sequences\ntext_data = vectorizer(text_data)\n\n# Apply SMOTE to the text and label data\nsmote = SMOTE(random_state=42)\ntext_data_smote, label_data_smote = smote.fit_resample(text_data, label_data)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(text_data_smote, label_data_smote, test_size=0.2, random_state=42)\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    'epochs': [1, 3, 5, 10],\n    'dropout_rate': [0.2, 0.6],\n    'optimizer': ['adam', 'rmsprop'],\n    'batch_size': [32],\n    'lstm_units': [64, 128],\n    'dense_units': [32, 64]\n}\n\n# Create the KerasClassifier wrapper for the BiLSTM model\nbilstm_model = tf.keras.wrappers.scikit_learn.KerasClassifier(\n    build_fn=create_bilstm_model,\n    input_dim=len(vectorizer.get_vocabulary()),\n    output_dim=128,\n    mask_zero=True\n)\n\n# Use GridSearchCV to search for the best hyperparameters\ngrid = GridSearchCV(\n    bilstm_model,\n    param_grid=param_grid,\n    cv=2,\n    verbose=1,\n    n_jobs=-1\n)\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n\n# Fit the GridSearchCV object to the training data with early stopping\ngrid.fit(X_train, y_train, epochs=3, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n\nprint('Best parameters: ', grid.best_params_)\nprint('Best score: ', grid.best_score_)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T20:15:48.334504Z","iopub.execute_input":"2023-04-06T20:15:48.335071Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 64 candidates, totalling 128 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:75: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nEpoch 1/3\nEpoch 1/3\nEpoch 1/3\n4199/4199 [==============================] - 8587s 2s/step - loss: 0.4318 - accuracy: 0.7981 - val_loss: 0.3657 - val_accuracy: 0.8346\nEpoch 2/3\n4199/4199 [==============================] - 8670s 2s/step - loss: 0.4284 - accuracy: 0.8017 - val_loss: 0.3554 - val_accuracy: 0.8392\nEpoch 2/3\n4199/4199 [==============================] - 8685s 2s/step - loss: 0.4196 - accuracy: 0.8063 - val_loss: 0.3387 - val_accuracy: 0.8540\nEpoch 2/3\n4199/4199 [==============================] - 8708s 2s/step - loss: 0.4209 - accuracy: 0.8053 - val_loss: 0.3468 - val_accuracy: 0.8456\nEpoch 2/3\n  81/4199 [..............................] - ETA: 2:18:16 - loss: 0.3368 - accuracy: 0.8503","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfilename_bilstmmodel = 'filename_bilstmmodel.sav'\npickle.dump(model, open(filename_bilstmmodel, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:46:48.343422Z","iopub.execute_input":"2023-04-01T19:46:48.344005Z","iopub.status.idle":"2023-04-01T19:46:48.435510Z","shell.execute_reply.started":"2023-04-01T19:46:48.343891Z","shell.execute_reply":"2023-04-01T19:46:48.434326Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3158977848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename_bilstmmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'filename_bilstmmodel.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_bilstmmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]}]}